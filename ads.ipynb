{
 "cells": [
  {
   "source": [
    "# ADS Independent citation counter\n",
    "\n",
    "In this notebook, I query ADS for independent citations to all of my peer-reviewed publications. I do this by looping over all refereed papers and excluding all authors of that paper from the query. We match on the last name and initials (as does ADS) and use ORCID iD for cross-checking whenever available."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sun Jul 11 00:39:11 EDT 2021\n"
     ]
    }
   ],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter"
   ]
  },
  {
   "source": [
    "#### Enter your API key below. \n",
    "\n",
    "To obtain an API key, you need to be logged in to your ADS account. Sign up for one with your email if you have never logged in. Go to **Account** > **Settings** > **API token** and copy your unique 40-character API key and assign it to the `token` variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "token=\"yOuR-kEy-HeRe\"\n",
    "headers={'Authorization': 'Bearer ' + token}\n",
    "\n",
    "check_orcid = True\n",
    "verbose = False  # Set it to True if you want lengthy outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a convenience function to standardize the names\n",
    "def standardize_names(name):\n",
    "    try:\n",
    "        lastname, firstname = auth.split(',')\n",
    "        initial = firstname[1]\n",
    "        return f\"{lastname},+{initial}\"\n",
    "    except:\n",
    "        # This usually happens for collaboration papers\n",
    "        print(f\"Cannot split '{name}' into first and last names!\")\n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, query for all refereed papers authored by kannawad*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "URL = https://api.adsabs.harvard.edu/v1/search/query?q=author%3Akannawad%2A&fq=property%3Arefereed+property%3Aarticle&fl=author,title,bibcode,citation_count,orcid_pub,orcid_user,orcid_other&rows=500\n",
      "\n",
      "CPU times: user 27.4 ms, sys: 8.99 ms, total: 36.4 ms\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "author='kannawad%2A' #kannawad*\n",
    "\n",
    "base_url = \"https://api.adsabs.harvard.edu/v1/search/query?\"\n",
    "fq = \"property%3Arefereed+property%3Aarticle\"\n",
    "fl = \"author,title,bibcode,citation_count,orcid_pub,orcid_user,orcid_other\"\n",
    "rows = 500\n",
    "\n",
    "# the query parameters can be included as part of the URL\n",
    "url = f\"{base_url}q=author%3A{author}&fq={fq}&fl={fl}&rows={rows}\"\n",
    "print(f\"URL = {url}\\n\")\n",
    "r = requests.get(url, headers=headers)\n",
    "# the requests package returns an object; to get just the JSON API response, you have to specify this\n",
    "js = r.json()\n",
    "publications = js['response']['docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete list of queried papers\n\t   Bibcode       #Cites \t Title\n\n2020A&A...633A..69H \t 186 \t KiDS+VIKING-450: Cosmic shear tomography with optical and infrared data\n2014ApJS..212....5M \t 118 \t The Third Gravitational Lensing Accuracy Testing (GREAT3) Challenge Handbook\n2021A&A...646A.140H \t 118 \t KiDS-1000 Cosmology: Multi-probe weak gravitational lensing and spectroscopic galaxy clustering constraints\n2020A&A...638L...1J \t 90 \t KiDS+VIKING-450 and DES-Y1 combined: Cosmology with cosmic shear\n2021A&A...645A.104A \t 97 \t KiDS-1000 cosmology: Cosmic shear constraints and comparison between two point statistics\n2020A&A...633L..10T \t 54 \t Cosmology from large-scale structure. Constraining ΛCDM with BOSS\n2020A&A...634A.127A \t 57 \t KiDS+VIKING-450 and DES-Y1 combined: Mitigating baryon feedback uncertainty with COSEBIs\n2019A&A...624A..92K \t 40 \t Towards emulating cosmic shear data: revisiting the calibration of the shear measurements for the Kilo-Degree Survey\n2018MNRAS.481.1337H \t 27 \t Cosmological simulations for combined-probe analyses: covariance and neighbour-exclusion bias\n2021A&A...646A.129J \t 33 \t KiDS-1000 methodology: Modelling and inference for joint weak gravitational lensing and spectroscopic galaxy clustering analysis\n2020A&A...640L..14W \t 28 \t KiDS+VIKING-450: Improved cosmological parameter constraints from redshift calibration with self-organising maps\n2018MNRAS.477.4285A \t 22 \t KiDS-i-800: comparing weak gravitational lensing measurements from same-sky surveys\n2021A&A...645A.105G \t 24 \t KiDS-1000 catalogue: Weak gravitational lensing shear measurements\n2021A&A...647A.124H \t 24 \t KiDS-1000 catalogue: Redshift distributions and their calibration\n2021A&A...649A..88T \t 21 \t KiDS-1000 Cosmology: Constraints beyond flat ΛCDM\n2019MNRAS.487.3715V \t 10 \t Luminous red galaxies in the Kilo-Degree Survey: selection with broad-band photometry and weak lensing measurements\n2016PASP..128i5001K \t 14 \t The Impact of Interpixel Capacitance in CMOS Detectors on PSF Shapes and Implications for WFIRST\n2019A&A...622A..90G \t 13 \t The dependence of intrinsic alignment of galaxies on wavelength using KiDS and GAMA\n2020A&A...642A.158B \t 12 \t Testing gravity using galaxy-galaxy lensing and clustering amplitudes in KiDS-1000, BOSS, and 2dFLenS\n2020A&A...633A..89X \t 8 \t A gravitational lensing detection of filamentary structures connecting luminous red galaxies\n2016PASP..128j4001P \t 10 \t The Effect of Detector Nonlinearity on WFIRST PSF Profiles for Weak Gravitational Lensing Measurements\n2020A&A...635A.139E \t 9 \t Euclid preparation. VI. Verifying the performance of cosmic shear experiments\n2015MNRAS.449.3597K \t 9 \t The impact of cosmic variance on simulating weak lensing surveys\n2020A&A...642A..83D \t 5 \t KiDS+GAMA: The weak lensing calibrated stellar-to-halo mass relation of central and satellite galaxies\n2020A&A...640A..59L \t 3 \t KiDS+VIKING+GAMA: Testing semi-analytic models of galaxy evolution with galaxy-galaxy-galaxy lensing\n2016EL....11557005K \t 3 \t Persistent entanglement in a class of eigenstates of quantum Heisenberg spin glasses\n2021A&A...649A..81N \t 3 \t Photometric selection and redshifts for quasars in the Kilo-Degree Survey Data Release 4\n2021A&A...646A..73S \t 3 \t Tightening weak lensing constraints on the ellipticity of galaxy-scale dark matter haloes\n2021A&A...649A.146R \t 3 \t Strong detection of the CMB lensing and galaxy weak lensing cross-correlation from ACT-DR4, Planck Legacy, and KiDS-1000\n2021A&A...646A.124H \t 1 \t Accounting for object detection bias in weak gravitational lensing studies\n2020A&A...638C...2E \t 1 \t Euclid preparation. VI. Verifying the Performance of Cosmic Shear Experiments (Corrigendum)\n2021MNRAS.502.4048K \t 1 \t Mitigating the effects of undersampling in weak lensing shear estimation with metacalibration\n2021A&A...646A.175L \t 1 \t KiDS+VIKING-450: An internal-consistency test for cosmic shear tomography with a colour-based split of source galaxies\n2021A&A...647A.185G \t 0 \t Halo shapes constrained from a pure sample of central galaxies in KiDS-1000\n2021A&A...650A.113B \t 0 \t The weak lensing radial acceleration relation: Constraining modified gravity and cold dark matter theories with KiDS-1000\n\nTotal number of citations =  1048\n"
     ]
    }
   ],
   "source": [
    "## Print the complete list of queried papers\n",
    "print(\"Complete list of queried papers\")\n",
    "print(\"\\t   Bibcode       #Cites \\t Title\\n\")\n",
    "citation_counts = 0\n",
    "for paper in publications:\n",
    "    print(paper['bibcode'], '\\t', paper['citation_count'], '\\t', paper['title'][0])\n",
    "    citation_counts += paper['citation_count']\n",
    "print(\"\\nTotal number of citations = \", citation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Ensure that the Orcid iDs are consistent and create a single orcid key per author\n",
    "for paper in publications:\n",
    "    nauth = len(paper['author'])\n",
    "    paper['orcid'] = []\n",
    "    for aid in range(nauth):\n",
    "        orcid = (paper['orcid_pub'][aid], paper.get('orcid_user','-'*nauth)[aid], paper.get('orcid_other','-'*nauth)[aid])\n",
    "        orcid = [orc for orc in orcid if orc!='-']\n",
    "        if len(orcid):\n",
    "            all((orc==orcid[0] for orc in orcid))\n",
    "            paper['orcid'].append(orcid[0])\n",
    "        else:\n",
    "            paper['orcid'].append('-')\n",
    "        if verbose:\n",
    "            print(paper['author'][aid], paper['orcid_pub'][aid], paper.get('orcid_user','-'*nauth)[aid], paper.get('orcid_other','-'*nauth)[aid])\n",
    "            print(paper['author'][aid], paper.get('orcid')[aid])\n"
   ]
  },
  {
   "source": [
    "This would have been an excellent exercise to use a mini database, with a Publications table and an Authors table. But for my purpose, this seemed like an overkill and I use basic Python structures like `dict` and `list`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cannot split 'Euclid Collaboration' into first and last names!\nCannot split 'Euclid Collaboration' into first and last names!\n"
     ]
    }
   ],
   "source": [
    "## Obtain the complete list of co-authors (including self), along with their unique ORCID iDs.\n",
    "coauthors = {}\n",
    "for paper in publications:\n",
    "    for auth, orc in zip(paper['author'], paper['orcid']):\n",
    "        std_name = standardize_names(auth)\n",
    "        coauthors[std_name] = orc if orc!='-' else coauthors.get(std_name, None)\n",
    "if verbose:\n",
    "    print(\"\\nName of the co-authors \\t\\t ORCID iD \\n \")\n",
    "    for coauth in sorted(coauthors.keys()):\n",
    "        print(coauth, '\\t\\t', coauthors[coauth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, many of my co-authors have not provided their ORCID iD. We will have to find common authors by name. ORCID iD will only serve as further validation where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The following authors have the same last name:\nChoi,+A \t None\nTaylor,+A \t None\nTaylor,+E \t 0000-0002-5522-9107\nChoi,+S \t None\n"
     ]
    }
   ],
   "source": [
    "print(\"The following authors have the same last name:\")\n",
    "cntr = Counter([name.split(',')[0] for name in coauthors.keys()])\n",
    "for coauth in coauthors:\n",
    "    if cntr[coauth.split(',')[0]] > 1:\n",
    "        print(coauth, '\\t', coauthors[coauth])"
   ]
  },
  {
   "source": [
    "Define an empty dictionary to contain the number of independent citations. Loop over the above publications, query the independent citations and count them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cannot split 'Euclid Collaboration' into first and last names!\n",
      "Cannot split 'Euclid Collaboration' into first and last names!\n",
      "CPU times: user 665 ms, sys: 52.8 ms, total: 718 ms\n",
      "Wall time: 8.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fq = \"property%3Aarticle\"\n",
    "fl = \"author,title,bibcode\"\n",
    "rows = 500\n",
    "for eid, entity in enumerate(js['response']['docs']):\n",
    "    independent_authors, independent_orcid = [], []\n",
    "    bibcode = entity['bibcode'].replace('&','%26')\n",
    "    condition = ''\n",
    "    for auth in entity['author']:\n",
    "        name = standardize_names(auth)\n",
    "        if name is None: continue\n",
    "        condition+= f\"+-author:%22{name}%22\"\n",
    "    url = f\"{base_url}q=citations(bibcode%3A{bibcode}){condition}&fq={fq}&fl={fl}&rows={rows}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    jstmp = r.json()\n",
    "    independent_authors += [standardize_names(paper['author']) for paper in jstmp['response']['docs']]\n",
    "    nauth = len(entity['author'])\n",
    "    if check_orcid:\n",
    "        # Get all coauthor orcid ids\n",
    "        coauthor_orcid = entity.get('orcid_pub',['-'])+entity.get('orcid_user',['-'])+entity.get('orcid_other',['-'])\n",
    "        for cite in jstmp['response']['docs']:\n",
    "            independent_orcid += cite.get('orcid_user',['-'])+cite.get('orcid_pub',['-'])+cite.get('orcid_other',['-'])\n",
    "\n",
    "        independent_orcid = set(independent_orcid)\n",
    "        coauthor_orcid = set(coauthor_orcid)\n",
    "        assert(independent_orcid.intersection(coauthor_orcid).issubset({'-'}))\n",
    "\n",
    "    js['response']['docs'][eid]['independent_citation_count'] = jstmp['response']['numFound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete list of queried papers\n\t   Bibcode       #Cites #Independent cites \t Title\n\n2020A&A...633A..69H \t 186 116 \t KiDS+VIKING-450: Cosmic shear tomography with optical and infrared data\n2014ApJS..212....5M \t 118 63 \t The Third Gravitational Lensing Accuracy Testing (GREAT3) Challenge Handbook\n2021A&A...646A.140H \t 118 72 \t KiDS-1000 Cosmology: Multi-probe weak gravitational lensing and spectroscopic galaxy clustering constraints\n2020A&A...638L...1J \t 90 69 \t KiDS+VIKING-450 and DES-Y1 combined: Cosmology with cosmic shear\n2021A&A...645A.104A \t 97 71 \t KiDS-1000 cosmology: Cosmic shear constraints and comparison between two point statistics\n2020A&A...633L..10T \t 54 35 \t Cosmology from large-scale structure. Constraining ΛCDM with BOSS\n2020A&A...634A.127A \t 57 32 \t KiDS+VIKING-450 and DES-Y1 combined: Mitigating baryon feedback uncertainty with COSEBIs\n2019A&A...624A..92K \t 40 14 \t Towards emulating cosmic shear data: revisiting the calibration of the shear measurements for the Kilo-Degree Survey\n2018MNRAS.481.1337H \t 27 12 \t Cosmological simulations for combined-probe analyses: covariance and neighbour-exclusion bias\n2021A&A...646A.129J \t 33 9 \t KiDS-1000 methodology: Modelling and inference for joint weak gravitational lensing and spectroscopic galaxy clustering analysis\n2020A&A...640L..14W \t 28 16 \t KiDS+VIKING-450: Improved cosmological parameter constraints from redshift calibration with self-organising maps\n2018MNRAS.477.4285A \t 22 2 \t KiDS-i-800: comparing weak gravitational lensing measurements from same-sky surveys\n2021A&A...645A.105G \t 24 10 \t KiDS-1000 catalogue: Weak gravitational lensing shear measurements\n2021A&A...647A.124H \t 24 7 \t KiDS-1000 catalogue: Redshift distributions and their calibration\n2021A&A...649A..88T \t 21 7 \t KiDS-1000 Cosmology: Constraints beyond flat ΛCDM\n2019MNRAS.487.3715V \t 10 3 \t Luminous red galaxies in the Kilo-Degree Survey: selection with broad-band photometry and weak lensing measurements\n2016PASP..128i5001K \t 14 3 \t The Impact of Interpixel Capacitance in CMOS Detectors on PSF Shapes and Implications for WFIRST\n2019A&A...622A..90G \t 13 1 \t The dependence of intrinsic alignment of galaxies on wavelength using KiDS and GAMA\n2020A&A...642A.158B \t 12 5 \t Testing gravity using galaxy-galaxy lensing and clustering amplitudes in KiDS-1000, BOSS, and 2dFLenS\n2020A&A...633A..89X \t 8 4 \t A gravitational lensing detection of filamentary structures connecting luminous red galaxies\n2016PASP..128j4001P \t 10 5 \t The Effect of Detector Nonlinearity on WFIRST PSF Profiles for Weak Gravitational Lensing Measurements\n2020A&A...635A.139E \t 9 4 \t Euclid preparation. VI. Verifying the performance of cosmic shear experiments\n2015MNRAS.449.3597K \t 9 4 \t The impact of cosmic variance on simulating weak lensing surveys\n2020A&A...642A..83D \t 5 3 \t KiDS+GAMA: The weak lensing calibrated stellar-to-halo mass relation of central and satellite galaxies\n2020A&A...640A..59L \t 3 1 \t KiDS+VIKING+GAMA: Testing semi-analytic models of galaxy evolution with galaxy-galaxy-galaxy lensing\n2016EL....11557005K \t 3 1 \t Persistent entanglement in a class of eigenstates of quantum Heisenberg spin glasses\n2021A&A...649A..81N \t 3 1 \t Photometric selection and redshifts for quasars in the Kilo-Degree Survey Data Release 4\n2021A&A...646A..73S \t 3 0 \t Tightening weak lensing constraints on the ellipticity of galaxy-scale dark matter haloes\n2021A&A...649A.146R \t 3 1 \t Strong detection of the CMB lensing and galaxy weak lensing cross-correlation from ACT-DR4, Planck Legacy, and KiDS-1000\n2021A&A...646A.124H \t 1 0 \t Accounting for object detection bias in weak gravitational lensing studies\n2020A&A...638C...2E \t 1 1 \t Euclid preparation. VI. Verifying the Performance of Cosmic Shear Experiments (Corrigendum)\n2021MNRAS.502.4048K \t 1 0 \t Mitigating the effects of undersampling in weak lensing shear estimation with metacalibration\n2021A&A...646A.175L \t 1 0 \t KiDS+VIKING-450: An internal-consistency test for cosmic shear tomography with a colour-based split of source galaxies\n2021A&A...647A.185G \t 0 0 \t Halo shapes constrained from a pure sample of central galaxies in KiDS-1000\n2021A&A...650A.113B \t 0 0 \t The weak lensing radial acceleration relation: Constraining modified gravity and cold dark matter theories with KiDS-1000\n\nTotal number of citations =  1048\n\nTotal number of independent citations =  572\n"
     ]
    }
   ],
   "source": [
    "## Print the complete list of queried papers, with independent citations counts for each\n",
    "print(\"Complete list of queried papers\")\n",
    "print(\"\\t   Bibcode       #Cites #Independent cites \\t Title\\n\")\n",
    "citation_counts = 0\n",
    "for paper in publications:\n",
    "    print(paper['bibcode'], '\\t', paper['citation_count'], paper['independent_citation_count'], '\\t', paper['title'][0])\n",
    "    citation_counts += paper['citation_count']\n",
    "print(\"\\nTotal number of citations = \", citation_counts)\n",
    "print(\"\\nTotal number of independent citations = \", sum([paper['independent_citation_count'] for paper in publications]))"
   ]
  },
  {
   "source": [
    "Voila! We have calculated independent citation counts for each referee publication of mine, and the total number of independent citations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('ssc--VxenSW5': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "interpreter": {
   "hash": "60cb5b6a0e0a40f12a0b6f394627794b0cb1d66db9335cabc448aaf203ffc8f4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}